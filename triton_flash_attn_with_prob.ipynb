{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f498e46-61cd-455e-b94a-7fff5f8847bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4f45e7-363d-46b2-9198-d3c5e73b8b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c787b01-180f-4119-892c-708501de741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_flash_with_p_bf16 import _attn_fwd_inner, _attn_fwd\n",
    "# from triton_flash_with_p_fp32 import _attn_fwd_inner, _attn_fwd\n",
    "# from triton_flash_with_p_fp16 import _attn_fwd_inner, _attn_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779b283c-aced-456a-a12c-53d89ff49bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, causal, sm_scale):\n",
    "    # shape constraints\n",
    "    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n",
    "    # when v is in float8_e5m2 it is transposed.\n",
    "    HEAD_DIM_V = v.shape[-2] if v.dtype == torch.float8_e5m2 else v.shape[-1]\n",
    "    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "    assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "    o = torch.empty_like(q)\n",
    "    stage = 3 if causal else 1\n",
    "    extra_kern_args = {}\n",
    "\n",
    "    # [seq / BLOCK_M, batch * head, 1]\n",
    "    grid = lambda args: (triton.cdiv(q.shape[2], args[\"BLOCK_M\"]), q.shape[0] * q.shape[1], 1)\n",
    "    # print(grid)\n",
    "    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32) # [batch, head, seq]\n",
    "    qk_max = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32) # [batch, head, seq]\n",
    "    qk_max_loc = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.int32) # [batch, head, seq]\n",
    "    # print(\"stage =\", stage)\n",
    "    _attn_fwd[grid](\n",
    "        q, k, v, sm_scale, M, qk_max, qk_max_loc, o,  #\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n",
    "        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n",
    "        q.shape[0], q.shape[1],  #\n",
    "        N_CTX=q.shape[2],  #\n",
    "        HEAD_DIM=HEAD_DIM_K,  #\n",
    "        STAGE=stage,  #\n",
    "        **extra_kern_args)\n",
    "    return o, M, qk_max, qk_max_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d203cec-4a8e-4bb0-886a-e014e9bc8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "Z = 1\n",
    "H = 2\n",
    "N_CTX = 1024\n",
    "HEAD_DIM = 64\n",
    "# dtype=torch.float32\n",
    "# dtype=torch.float16\n",
    "dtype=torch.bfloat16\n",
    "q = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
    "k = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
    "v = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=\"cuda\").normal_(mean=0.0, std=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa58f73-462a-4c83-935d-23c47e4fd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal = True\n",
    "sm_scale = 0.5\n",
    "tri_out, tri_M, tri_qk_max, qk_max_loc = attention(q, k, v, causal, sm_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8025f3d-fc81-4214-a782-182519ef2fb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 8.5449e-03, -6.6895e-02, -2.7148e-01,  ..., -2.8125e-01,\n",
       "            5.7422e-01,  2.3242e-01],\n",
       "          [-3.0273e-01, -1.0645e-01, -2.2754e-01,  ..., -1.0376e-02,\n",
       "            4.6094e-01, -2.2754e-01],\n",
       "          [ 3.9551e-02, -1.4258e-01, -2.6953e-01,  ..., -2.4023e-01,\n",
       "            5.0391e-01,  1.4453e-01],\n",
       "          ...,\n",
       "          [-9.2163e-03, -2.4536e-02, -2.2278e-03,  ...,  1.9043e-02,\n",
       "           -2.5757e-02,  2.9175e-02],\n",
       "          [-8.1177e-03, -2.5024e-02, -5.2185e-03,  ..., -2.2949e-02,\n",
       "           -1.3855e-02,  2.1851e-02],\n",
       "          [ 1.0742e-02, -1.8188e-02, -2.1820e-03,  ...,  1.6479e-02,\n",
       "           -1.0803e-02,  3.4142e-04]],\n",
       "\n",
       "         [[-6.3477e-02, -3.2031e-01, -6.7969e-01,  ...,  4.4727e-01,\n",
       "           -7.1484e-01, -3.5156e-01],\n",
       "          [-2.5879e-02, -1.3184e-01, -2.7734e-01,  ...,  1.6406e-01,\n",
       "           -3.5547e-01, -5.0000e-01],\n",
       "          [-6.8359e-02, -1.8945e-01, -3.7891e-01,  ...,  1.9922e-01,\n",
       "           -3.8281e-01, -4.4922e-01],\n",
       "          ...,\n",
       "          [ 3.1006e-02, -4.6387e-02,  6.4697e-03,  ..., -2.4048e-02,\n",
       "           -8.4229e-03, -2.4170e-02],\n",
       "          [ 7.8735e-03, -2.2339e-02, -1.1108e-02,  ...,  1.0681e-02,\n",
       "            2.6245e-02,  1.3367e-02],\n",
       "          [ 1.8188e-02, -2.0264e-02, -1.5747e-02,  ...,  1.6602e-02,\n",
       "            1.6357e-02, -1.9897e-02]]]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b8b38ca-3534-44f3-bccb-ac2ed91f054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch output\n",
    "M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n",
    "p_ = torch.matmul(q, k.transpose(2, 3))\n",
    "if causal:\n",
    "    p_[:, :, M == 0] = float(\"-inf\")\n",
    "ref_qk_max = p_.max(-1)\n",
    "\n",
    "p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n",
    "if causal:\n",
    "    p[:, :, M == 0] = float(\"-inf\")\n",
    "p = torch.softmax(p.float(), dim=-1).to(dtype)\n",
    "ref_out = torch.matmul(p, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d49087f-cd1b-4238-9da8-55647930d678",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 8.5449e-03, -6.6895e-02, -2.7148e-01,  ..., -2.8125e-01,\n",
       "            5.7422e-01,  2.3242e-01],\n",
       "          [-3.0273e-01, -1.0693e-01, -2.2852e-01,  ..., -1.0986e-02,\n",
       "            4.6289e-01, -2.2754e-01],\n",
       "          [ 3.9795e-02, -1.4258e-01, -2.6953e-01,  ..., -2.4121e-01,\n",
       "            5.0391e-01,  1.4551e-01],\n",
       "          ...,\n",
       "          [-9.0332e-03, -2.4658e-02, -2.6398e-03,  ...,  1.9165e-02,\n",
       "           -2.5879e-02,  2.9419e-02],\n",
       "          [-7.9956e-03, -2.5024e-02, -5.0964e-03,  ..., -2.2949e-02,\n",
       "           -1.3794e-02,  2.2095e-02],\n",
       "          [ 1.0681e-02, -1.7944e-02, -2.0294e-03,  ...,  1.6235e-02,\n",
       "           -1.0742e-02,  9.4414e-05]],\n",
       "\n",
       "         [[-6.3477e-02, -3.2031e-01, -6.7969e-01,  ...,  4.4727e-01,\n",
       "           -7.1484e-01, -3.5156e-01],\n",
       "          [-2.5879e-02, -1.3184e-01, -2.7734e-01,  ...,  1.6406e-01,\n",
       "           -3.5352e-01, -5.0000e-01],\n",
       "          [-6.8359e-02, -1.8945e-01, -3.7891e-01,  ...,  1.9922e-01,\n",
       "           -3.8281e-01, -4.4922e-01],\n",
       "          ...,\n",
       "          [ 3.1006e-02, -4.6387e-02,  6.3171e-03,  ..., -2.4170e-02,\n",
       "           -8.3008e-03, -2.4170e-02],\n",
       "          [ 7.9346e-03, -2.2339e-02, -1.1108e-02,  ...,  1.0559e-02,\n",
       "            2.6367e-02,  1.3428e-02],\n",
       "          [ 1.8066e-02, -2.0386e-02, -1.5137e-02,  ...,  1.6602e-02,\n",
       "            1.6724e-02, -2.0142e-02]]]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c54c7de8-0735-47f2-bedc-7b8218851d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triton flash attn in bf16 v.s. standard attn in bf16\n",
    "qk_max_loc.size(2) - (qk_max_loc[0, 0] == ref_qk_max.indices[0, 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bd762e7-5b2c-4dc7-9f72-deb328bd8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n",
    "p_fp32 = torch.matmul(q.float(), k.float().transpose(2, 3))\n",
    "if causal:\n",
    "    p_fp32[:, :, M == 0] = float(\"-inf\")\n",
    "ref_qk_max_fp32 = p_fp32.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f15c67c-dbf4-49c6-86cd-72bdd3976bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard attn in fp 32 v.s. standard attn in bf16\n",
    "# check the issue: https://github.com/Dao-AILab/flash-attention/issues/383\n",
    "qk_max_loc.size(2) - (ref_qk_max_fp32.indices[0, 0] == ref_qk_max.indices[0, 0]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
